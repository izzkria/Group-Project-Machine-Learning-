# -*- coding: utf-8 -*-
"""ML project manufacturing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ibn0R6PMqLLwqlhZrzEad5ZM4VO04pNZ

# **BSD3523 MACHINE LEARNING GROUP PROJECT**

**Group Name:** CSM1

**Group Leader:**

YIP YOONG ENG (SD23048)

**Group Members:**

MUHAMMAD AMIRUL AMIER BIN MOHD HUSNI (SD23011)

ALIYA AFIFAH BINTI AL ABAS (SD23062)

NUR IZZATI BINTI ZAKARIA (SD23007)

ALIA AYUNNI BINTI MOHD SHUKRI (SD23054)

## **IMPORT LIBRARIES**
"""

import pandas as pd
import numpy as np
import seaborn as sns
import datetime
import time
import glob
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split

from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor

from sklearn.pipeline import make_pipeline
from sklearn.model_selection import GridSearchCV

from sklearn.metrics import mean_squared_error, mean_absolute_error,r2_score
from sklearn.metrics import accuracy_score, roc_curve,auc,roc_auc_score
from sklearn.metrics import confusion_matrix, classification_report

"""Load the files from this [Google Drive Link](https://drive.google.com/drive/folders/1_DjDZ2Z1uY_APF-StLr__zUMaGtHbM-D?usp=drive_link) and upload accordingly."""

from google.colab import files
uploaded1 = files.upload()

from google.colab import files
uploaded2 = files.upload()

from google.colab import files
uploaded3 = files.upload()

from google.colab import files
uploaded4 = files.upload()

from google.colab import files
uploaded5 = files.upload()

from google.colab import files
uploaded6 = files.upload()

from google.colab import files
uploaded7 = files.upload()

"""## **DATA INTEGRATION (MERGE SUPPORTING DATA AND MAIN DATA)**"""

weather_df = pd.read_csv('weather.csv')
weather_df['Date'] = pd.to_datetime(weather_df['Date'])

price2020_df = pd.read_csv('price_2020.csv')
price2020_df['Date'] = pd.to_datetime(price2020_df['Date'])

price2021_df = pd.read_csv('price_2021.csv')
price2021_df['Date'] = pd.to_datetime(price2021_df['Date'])

price2022_df = pd.read_csv('price_2022.csv')
price2022_df['Date'] = pd.to_datetime(price2022_df['Date'])

ipi_df = pd.read_csv('production_index.csv')
ipi_df['Date'] = pd.to_datetime(ipi_df['Date'])

export_df = pd.read_csv('export_number.csv')
export_df['Date'] = pd.to_datetime(export_df['Date'])

exchange_df = pd.read_csv('exchange_rates.csv')
exchange_df['Date'] = pd.to_datetime(exchange_df['Date'], format='%d-%m-%y')

weather_df.tail(5)

price2020_df.head(5)

price2021_df.head(5)

price2022_df.head(5)

price2022_df.tail()

ipi_df.head(5)

export_df.tail(5)

exchange_df.head(5)

"""As you can see, the dates like January 4th and 5th, 2020 now have values, which were forward-filled from January 3rd's value. This ensures all dates have an exchange rate.

Since exchange rates is reported daily except for weekends, the weekend dates will be replaced with same value with Friday of the week. Whereas production index, and oil palm exports are reported as monthly data, the dates are converted to a daily frequency.
"""

# Reset index if 'Date' is already the index to make it a column again
if 'Date' not in exchange_df.columns:
    exchange_df.reset_index(inplace=True)

# 'Date' column is already parsed correctly in the previous step, so no need to re-parse.
exchange_df.set_index('Date',inplace=True)

# conduct all date range in dataset
full_index = pd.date_range(
    start=exchange_df.index.min(),
    end=exchange_df.index.max(),
    freq='D'
)

# remove duplicate index entries before reindexing
exchanges_expanded = exchange_df[~exchange_df.index.duplicated(keep='first')]

exchanges_expanded = exchanges_expanded.reindex(full_index)

# ensure daily frequency
exchanges_expanded = exchanges_expanded.asfreq('D')

# forward fill missing days
exchanges_expanded = exchanges_expanded.ffill()

# Restore Date column
exchanges_expanded.index.name = 'Date'
exchanges_expanded = exchanges_expanded.reset_index()

exchanges_expanded.tail(5)

export_df['Date'] = pd.to_datetime(export_df['Date'], errors='coerce')
export_df.dropna(subset=['Date'], inplace=True)
export_df['year'] = export_df['Date'].dt.year
export_df['month'] = export_df['Date'].dt.month
export_expanded = ( export_df .assign(key=1) .merge(pd.DataFrame({'day': range(1, 32), 'key': 1}), on='key', how='left') .drop('key', axis=1) )
export_expanded['Date'] = pd.to_datetime( export_expanded['year'].astype(str) + '-' + export_expanded['month'].astype(str) + '-' + export_expanded['day'].astype(str), errors='coerce' )
export_expanded = export_expanded[export_expanded['Date'] <= '2022-08-25']
export_expanded = export_expanded[['Date', 'Export Number (in Tonnes)']]

export_expanded.tail(5)

ipi_df['Date'] = pd.to_datetime(ipi_df['Date'], errors='coerce')
ipi_df.dropna(subset=['Date'], inplace=True)

ipi_df['year'] = ipi_df['Date'].dt.year
ipi_df['month'] = ipi_df['Date'].dt.month

ipi_expanded = (
    ipi_df
    .assign(key=1)
    .merge(pd.DataFrame({'day': range(1, 32), 'key':1}), on='key', how='left')
    .drop('key', axis=1)
)

ipi_expanded['Date'] = pd.to_datetime(
    ipi_expanded['year'].astype(str) + '-' +
    ipi_expanded['month'].astype(str) + '-' +
    ipi_expanded['day'].astype(str),
    errors='coerce'
)

ipi_expanded = ipi_expanded[ipi_expanded['Date'] <= '2022-08-25']
ipi_expanded = ipi_expanded[['Date', 'Index Production']]
ipi_expanded = ipi_expanded.drop_duplicates(subset=['Date']).reset_index(drop=True)

ipi_expanded.tail(5)

price2020_df['Date'] = pd.to_datetime(price2020_df['Date'], errors='coerce')
price2020_df.dropna(subset=['Date'], inplace=True)

price2020_df['year'] = price2020_df['Date'].dt.year
price2020_df['month'] = price2020_df['Date'].dt.month

months = pd.DataFrame({'month': range(1, 13)})
months['year'] = 2020


days = pd.DataFrame({'day': range(1, 32)})
expanded = months.merge(days, how='cross')  # cross join to get all day-month combinations

expanded['Date'] = pd.to_datetime(
    expanded['year'].astype(str) + '-' +
    expanded['month'].astype(str) + '-' +
    expanded['day'].astype(str),
    errors='coerce'
)

expanded = expanded[['Date']].dropna()

price2020_expanded = pd.merge(
    expanded,
    price2020_df[['Date', 'Price']],
    on='Date',
    how='left'
)

price2020_expanded = price2020_expanded.sort_values('Date').reset_index(drop=True)

price2020_expanded.tail(5)

price2021_df['Date'] = pd.to_datetime(price2021_df['Date'], errors='coerce')
price2021_df.dropna(subset=['Date'], inplace=True)

price2021_df['year'] = price2021_df['Date'].dt.year
price2021_df['month'] = price2021_df['Date'].dt.month

months = pd.DataFrame({'month': range(1, 13)})
months['year'] = 2021

days = pd.DataFrame({'day': range(1, 32)})
expanded = months.merge(days, how='cross')

expanded['Date'] = pd.to_datetime(
    expanded['year'].astype(str) + '-' +
    expanded['month'].astype(str) + '-' +
    expanded['day'].astype(str),
    errors='coerce'
)

expanded = expanded[['Date']].dropna()
price2021_expanded = pd.merge(
    expanded,
    price2021_df[['Date', 'Price']],
    on='Date',
    how='left'
)

price2021_expanded = price2021_expanded[['Date', 'Price']]
price2021_expanded = price2021_expanded.sort_values('Date').reset_index(drop=True)

price2021_expanded.tail(5)

price2022_df['Date'] = pd.to_datetime(price2022_df['Date'], errors='coerce')
price2022_df.dropna(subset=['Date'], inplace=True)

price2022_df['year'] = price2022_df['Date'].dt.year
price2022_df['month'] = price2022_df['Date'].dt.month

months = pd.DataFrame({'month': range(1, 13)})
months['year'] = 2022


days = pd.DataFrame({'day': range(1, 32)})
expanded = months.merge(days, how='cross')

expanded['Date'] = pd.to_datetime(
    expanded['year'].astype(str) + '-' +
    expanded['month'].astype(str) + '-' +
    expanded['day'].astype(str),
    errors='coerce'
)

price2022_expanded = pd.merge(
    expanded,
    price2022_df[['Date', 'Price']],
    on='Date',
    how='left'
)

price2022_expanded = price2022_expanded[price2022_expanded['Date'] <= '2022-08-25']
price2022_expanded = price2022_expanded.sort_values('Date').reset_index(drop=True)
price2022_expanded = price2022_expanded[['Date', 'Price']]

price2022_expanded.tail(5)

"""Now, all the dataframes are presented at a daily frequency, where the values of exchange, IPI, and export are expanded to each day, so that each day has the same value. This allows the datasets to be combined more easily with other dataframes."""

# Combine all price dataframes vertically
all_prices_df = pd.concat([
    price2020_expanded,
    price2021_expanded,
    price2022_expanded
], ignore_index=True)

# Sort by Date and reset index for a clean combined DataFrame
all_prices_df = all_prices_df.sort_values(by='Date').reset_index(drop=True)

# Display the head of the combined DataFrame
print("Combined Price DataFrame (all_prices_df):")
all_prices_df.tail()

# Reset index if 'Date' is already the index to make it a column again
if 'Date' not in all_prices_df.columns:
    all_prices_df.reset_index(inplace=True)

# 'Date' column is already parsed correctly in the previous step, so no need to re-parse.
all_prices_df.set_index('Date',inplace=True)

# conduct all date range in dataset
full_index = pd.date_range(
    start=all_prices_df.index.min(),
    end=all_prices_df.index.max(),
    freq='D'
)

# remove duplicate index entries before reindexing
all_prices_df = all_prices_df[~all_prices_df.index.duplicated(keep='first')]

all_prices_df = all_prices_df.reindex(full_index)

# ensure daily frequency
all_prices_df = all_prices_df.asfreq('D')

# forward fill missing days
all_prices_df = all_prices_df.ffill()

# Restore Date column
all_prices_df.index.name = 'Date'
all_prices_df = all_prices_df.reset_index()

all_prices_df.tail(7)

df = weather_df.copy()
df = pd.merge(df, ipi_expanded, on='Date', how='left')
df = pd.merge(df, all_prices_df, on='Date', how='left')
df = pd.merge(df, export_expanded, on='Date', how='left')
df = pd.merge(df, exchanges_expanded, on='Date', how='left')

df

print(df.columns)

"""Data is loaded and named as **df**, to be used in data preprocessing.

**CHECK DATA TYPE FOR DATASET**
"""

df.dtypes

"""Export Number variables supposedly in numerical format rather in object (string) data type. Therefore, the data type must be revised."""

numeric_cols = ['Export Number (in Tonnes)']

for col in numeric_cols:
    if col in df.columns:
        df[col] = df[col].astype(str).str.replace(',', '', regex=False)
        df[col] = pd.to_numeric(df[col], errors='coerce')

df.dtypes

"""All variables are numerical except for the date variable. Therefore, if there are missing values in this dataset, we can replaced using the mean or median, which can only be calculated from numerical data.

## **DATA PREPROCESSING**

**CHECK MISSING VALUES AND DUPLICATES**
"""

df.isnull().sum()

df.shape

df.duplicated().sum()

"""There is no duplicated rows in the data but missing values exist.

**MISSING VALUE IMPUTATION**
"""

import matplotlib.pyplot as plt

for col in ['Index Production', 'Price', 'Export Number (in Tonnes)', 'USD']:
    plt.figure()
    df[col].hist(bins=30)
    # Clean, simple titles
    title = col.replace('(in Tonnes)', '').replace('Index Production', 'IPI')
    plt.title(title)
    plt.xlabel(title)
    plt.ylabel('Frequency')
    plt.show()

"""The distribution of all relevant variables (Price, Export Number (in Tonnes), Index Production, USD) are examined. They shows skewness and do not follow a perfect bell shape. Therefore, we decided to replace missing values with the median because the median is robust to skewed distributions and outliers, ensuring a more reliable imputation."""

# Set Date as index for time series operations
df_imputed = df.set_index('Date').copy()

# Column to impute with rolling mean
col_to_roll_impute = 'Export Number (in Tonnes)'

# Apply rolling mean imputation for 'Export Number (in Tonnes)'
# Calculate 30-day rolling mean
# min_periods=1 ensures a mean is calculated even if fewer than 30 values are available (e.g., at the start of the series)
rolling_mean = df_imputed[col_to_roll_impute].rolling(window=30, min_periods=1).mean()
# Fill NaN values with rolling mean
df_imputed[col_to_roll_impute] = df_imputed[col_to_roll_impute].fillna(rolling_mean)
# Fallback to median for any remaining NaNs (e.g., if first 30 days are all NaN)
df_imputed[col_to_roll_impute] = df_imputed[col_to_roll_impute].fillna(df_imputed[col_to_roll_impute].median())

# Columns to impute with median (reverting others)
cols_to_median_impute = ['Price', 'Index Production', 'USD']
for col in cols_to_median_impute:
    df_imputed[col] = df_imputed[col].fillna(df_imputed[col].median())

# Reset index to make 'Date' a column again
df = df_imputed.reset_index()

# Handle Sealevelpressure as before
df = df.dropna(subset=['Sealevelpressure'])

df.isnull().sum()

"""Median was choosen to replace missing values because of their robust to outliers and is appropiate for skewed data.

We decided to drop missing value in variable Sealevelpressure, since the missing value less than 5.
"""

df

"""## **EXPLORATORY DATA ANALYSIS**

**Descriptive Statistics**
"""

display(df.describe(include='all'))

sns.set_style("darkgrid")

numerical_columns = df.select_dtypes(include=["int64", "float64"])

numerical_columns.hist(figsize=(14, 18), bins=20)

plt.suptitle("Distribution of Numerical Variables", fontsize=16)
plt.tight_layout()
plt.show()

sns.set_style("darkgrid")

numerical_columns = df.select_dtypes(include=["int64", "float64"]).columns

plt.figure(figsize=(14, len(numerical_columns) * 3))
for idx, feature in enumerate(numerical_columns, 1):
    plt.subplot(len(numerical_columns), 2, idx)
    sns.histplot(df[feature], kde=True)
    plt.title(f"{feature} | Skewness: {round(df[feature].skew(), 2)}")

plt.tight_layout()
plt.show()

"""**Correlation Matrix**"""

corr_matrix = df.drop(columns=['Date']).corr(numeric_only=True)

corr_matrix

corr = df.drop(columns=['Date']).corr(numeric_only=True)

plt.figure(figsize=(10,6))
sns.heatmap(corr, annot=True, fmt=".2f", cmap="coolwarm", linewidths=0.5)
plt.title("Correlation Heatmap")
plt.tight_layout()
plt.show()

"""**Box Plot (Outlier)**"""

plt.figure(figsize=(21, 7))
sns.boxplot(x=df['Price'])
plt.title("Boxplot of Price")
plt.tight_layout()
plt.show()

plt.figure(figsize=(21, 7))
sns.boxplot(x=df['Export Number (in Tonnes)'])
plt.title("Boxplot of Export Number (in Tonnes)")
plt.tight_layout()
plt.show()

plt.figure(figsize=(21, 7))
sns.boxplot(x=df['Index Production'])
plt.title("Boxplot of Index Production")
plt.tight_layout()
plt.show()

plt.figure(figsize=(21, 7))
sns.boxplot(x=df['USD'])
plt.title("Boxplot of USD")
plt.tight_layout()
plt.show()

df['Month'] = df['Date'].dt.month

plt.figure(figsize=(14, 7))
sns.boxplot(x='Month', y='Price', data=df)
plt.title("Price Distribution by Month")
plt.xlabel("Month")
plt.ylabel("Price")
plt.tight_layout()
plt.show()

df['Month'] = df['Date'].dt.month
df['Year'] = df['Date'].dt.year

month_year_dist = df.groupby(['Year', 'Month']).size().unstack().T

month_year_dist.plot(kind='bar', stacked=True, figsize=(14, 7))
plt.title('Stacked Bar Plot: Month-wise Distribution by Year')
plt.xlabel('Month')
plt.ylabel('Count')
plt.tight_layout()
plt.show()

# Select only numerical columns
numerical_columns = df.select_dtypes(include=["int64", "float64"]).columns

# Scatter plot matrix (Pairplot)
sns.pairplot(df[numerical_columns])
plt.suptitle('Pairwise Scatter Plot for All Numerical Variables', y=1.02)
plt.tight_layout()
plt.show()

"""**Time Series Comparison (Yearly Trends)**"""

# Use the already combined all_prices_df
combined_price_df = all_prices_df.copy()

# Apply forward-fill to propagate the last valid observation forward
combined_price_df['Price'] = combined_price_df['Price'].ffill()

# Drop rows where 'Price' is NaN after ffill, if any remain (e.g., at the very beginning)
combined_price_df.dropna(subset=['Price'], inplace=True)

# Add Year column for hue plotting
combined_price_df['Year'] = combined_price_df['Date'].dt.year

# Create the line plot
plt.figure(figsize=(15, 7))
sns.lineplot(data=combined_price_df, x='Date', y='Price', hue='Year', palette='viridis')
plt.title('Palm Oil Price Trends (2020-2022)')
plt.xlabel('Date')
plt.ylabel('Price')
plt.grid(True)
plt.legend(title='Year')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""## **FEATURE ENGINEERING**"""

df['Year'] = df['Date'].dt.year
df['Month'] = df['Date'].dt.month

X = df[['Temp', 'Dew', 'Humidity', 'Precip', 'Precipprob', 'Precipcover',
       'Windspeed', 'Winddir', 'Sealevelpressure', 'Cloudcover', 'Visibility',
       'Solarradiation', 'Solarenergy', 'Uvindex', 'Moonphase',
       'Index Production', 'Export Number (in Tonnes)', 'USD', 'Year',
       'Month']]
y = df['Price']

"""The dataset was divided into target(y) which is 'Price' and features(X)  which are all the variables except the column of 'Price'."""

corr = X.corrwith(y)
print(corr)

"""Shows that the strength and direction of the linear relationship between each feature and the target variable (export number)

Year has the strongest positive correlation with 0.6825, suggesting price tend to increase over time, possibly reflecting long term trends.

Index Production has moderate positive correlation with 0.4501,indicating that higher production levels are associated with higher prices of palm oil.

The UV index, Solar Radiation and Solar Energy variables show a weak porsitive correlations, with values of 0.1731, 0.1328 and 0.1338, respectively. This indicates that sunny conditions might slightly influence price of palm oil.

While, Dew and Sea Level Pressure with correlations -0.2181 and -0.1447 respectively will slightly correspond to lower prices of palm oil. This indicates that the higher moisture and pressure will result the low prices.

Variables such as Temperature, Humidity, windspeed and Precipitation have very weak negative correlations which are -0.0677, -0.1045, -0.0441 and -0.0182 respectively, indicating no affect to prices.

Variables such as Month, cloud cover, moon phase, precipitation cover, usd and wind direction, showing that these variables are irrelevant for predicting price since their correlation are very weak.

Therefore, the features such as 'Dew', 'Humidity', 'Sealevelpressure', 'Solarradiation', 'Solarenergy', 'Uvindex', 'Index Production', 'Export Number (in Tonnes)' and 'Year' have more impact on targeted variable (Price).

## **FEATURE SELECTION**
"""

from sklearn.feature_selection import f_classif

anova_results = []
for feature in X.columns:
  F, p = f_classif(X[[feature]], y)
  anova_results.append({'Feature': feature, 'F-value': F[0], 'p-value': p[0]})
anova_results = pd.DataFrame(anova_results).sort_values(by='p-value')
print("\nANOVA Results (Numerical Features):")
display(anova_results)

# Select features with p-value < 0.05 from ANOVA results
selected_features_anova = anova_results[anova_results['p-value'] < 0.05]['Feature'].tolist()

# Create X_selected using these features from the original X
X_selected = X[selected_features_anova]

X_selected.columns

selected_features = corr[abs(corr) >= 0.1].index
print(selected_features)

X_selected = X[selected_features]

"""## **PARTITIONING**

The dataset was split into training and testing sets with ratio 80:20.

X_train and y_train are used to to train the model while X_test and y_test are reserved for evaluating the model's performance on unseen data.
"""

X_train, X_test, y_train, y_test = train_test_split(
    X_selected, y, test_size=0.2, random_state=42
)

"""## **FEATURE SCALING**"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import RFE

# Initialize scaler and RFE estimator
scaler = StandardScaler()
estimator = LinearRegression()

X = df.drop(columns=['Price'])
y = df['Price']

scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""Then the features are standardized using StandardScaler. The training and testing set were scaled to have mean 0 and sd 1 to make sure all features are on the same scale to improve model performance and stability.

##  **Decision Tree Regressor**
"""

dt = DecisionTreeRegressor(
    max_depth=4,
    min_samples_leaf=10,
    random_state=42
)
start_time = time.time()
dt.fit(X_train_scaled, y_train)
end_time = time.time()
training_time_dt = end_time - start_time

dt_pred = dt.predict(X_test_scaled)

rmse_dt = np.sqrt(mean_squared_error(y_test, dt_pred))
r2_dt = r2_score(y_test, dt_pred)
mae_dt = mean_absolute_error(y_test, dt_pred)

print(f"Mean Squared Error (dt): {rmse_dt}")
print(f"MAE (dt): {mae_dt}")
print(f"R-squared (dt): {r2_dt}")

"""**Grid Search Tuning parameters for Decision Tree Regressor**"""

param_grid = {
       'criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],
       'max_depth': [None, 5, 10, 15, 20],
       'min_samples_split': [2, 5, 10],
       'min_samples_leaf': [1, 2, 4]
             }

grid_search = GridSearchCV(estimator=dt,
                              param_grid=param_grid,
                              cv=5,  # Number of cross-validation folds
                              scoring='neg_mean_squared_error',  # Evaluation metric
                              verbose=1)

grid_search.fit(X_train_scaled, y_train)

# Get the best parameters and best score
best_params_dt = grid_search.best_params_
best_score_dt = grid_search.best_score_
best_rmse_dt = np.sqrt(-best_score_dt)
best_dt_model=grid_search.best_estimator_
print(f"Best parameters for Decision Tree: {best_params_dt}")
print(f"Best cross-validation accuracy (Decision Tree): {best_rmse_dt:.4f}")

dt_tuned = DecisionTreeRegressor(
    max_depth=5,
    min_samples_leaf=4,
    min_samples_split=10,
    criterion='squared_error',
    random_state=42
)
start_time = time.time()
dt_tuned.fit(X_train_scaled, y_train)
end_time = time.time()
training_time_dt_tuned = end_time - start_time

dt_tuned_pred = dt.predict(X_test_scaled)

rmse_dt_tuned = np.sqrt(mean_squared_error(y_test, dt_tuned_pred))
r2_dt_tuned = r2_score(y_test, dt_tuned_pred)
mae_dt_tuned = mean_absolute_error(y_test, dt_tuned_pred)

print(f"Mean Squared Error (dt): {rmse_dt_tuned}")
print(f"MAE (dt): {mae_dt_tuned}")
print(f"R-squared (dt): {r2_dt_tuned}")

"""## **Random Forest Regressor**"""

rf = RandomForestRegressor(
    n_estimators=200,
    max_depth=5,
    min_samples_leaf=5,
    random_state=42
)
start_time = time.time()
rf.fit(X_train_scaled, y_train)
end_time = time.time()
training_time_rf = end_time - start_time

rf_pred = rf.predict(X_test_scaled)

rmse_rf = np.sqrt(mean_squared_error(y_test, rf_pred))
r2_rf = r2_score(y_test, rf_pred)
mae_rf = mean_absolute_error(y_test, rf_pred)

print(f"Mean Squared Error (RF): {rmse_rf}")
print(f"MAE: {mae_rf}")
print(f"R-squared (RF): {r2_rf}")

param_grid_rf = {
    "n_estimators": [100, 200],
    "max_depth": [5, 10, None],
    "min_samples_split": [2, 5],
    "min_samples_leaf": [1, 2]
}


grid_search = GridSearchCV(estimator=rf,
                           param_grid=param_grid_rf,
                           cv=5,  # Number of cross-validation folds
                           scoring='neg_mean_squared_error',
                           verbose=1)

grid_search.fit(X_train_scaled, y_train)
# Get the best parameters and best score
best_params_rf = grid_search.best_params_
best_score_rf = grid_search.best_score_
best_rmse_rf = np.sqrt(-best_score_rf)
best_rf_model=grid_search.best_estimator_
print(f"Best parameters for Random Forest: {best_params_rf}")
print(f"Best cross-validation accuracy (Random Forest): {best_rmse_rf:.4f}")

rf_tuned = RandomForestRegressor(
    n_estimators=best_params_rf['n_estimators'],
    max_depth=best_params_rf['max_depth'],
    min_samples_leaf=best_params_rf['min_samples_leaf'],
    min_samples_split=best_params_rf['min_samples_split'],
    random_state=42
)
start_time = time.time()
rf_tuned.fit(X_train_scaled, y_train)
end_time = time.time()
training_time_rf_tuned = end_time - start_time

rf_tuned_pred = rf_tuned.predict(X_test_scaled)

rmse_rf_tuned = np.sqrt(mean_squared_error(y_test, rf_tuned_pred))
r2_rf_tuned = r2_score(y_test, rf_tuned_pred)
mae_rf_tuned = mean_absolute_error(y_test, rf_tuned_pred)

print(f"Mean Squared Error (RF): {rmse_rf_tuned}")
print(f"MAE: {mae_rf_tuned}")
print(f"R-squared (RF): {r2_rf_tuned}")

"""## **XGBoost**"""

from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

xgb = XGBRegressor(
    n_estimators=200,
    max_depth=5,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)
start_time = time.time()
xgb.fit(X_train_scaled, y_train)
xgb_pred = xgb.predict(X_test_scaled)
end_time = time.time()
training_time_xgb = end_time - start_time

rmse_xgb = np.sqrt(mean_squared_error(y_test, xgb_pred))
mae_xgb = mean_absolute_error(y_test, xgb_pred)
r2_xgb = r2_score(y_test, xgb_pred)

print(f"RMSE (XGBoost): {rmse_xgb}")
print(f"MAE (XGBoost): {mae_xgb}")
print(f"R-squared (XGBoost): {r2_xgb}")

param_grid_xgb_reg = {
    'objective': ['reg:squarederror', 'reg:absoluteerror'],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1]
}
grid_search = GridSearchCV(estimator=xgb,
                              param_grid=param_grid_xgb_reg,
                              cv=5,  # Number of cross-validation folds
                              scoring='neg_mean_squared_error',  # Evaluation metric
                              verbose=1)

grid_search.fit(X_train_scaled, y_train)

# Get the best parameters and best score
best_params_xgb = grid_search.best_params_
best_score_xgb = grid_search.best_score_
best_rmse_xgb = np.sqrt(-best_score_xgb)
best_xgb_model=grid_search.best_estimator_
print(f"Best parameters for XGBoost: {best_params_xgb}")
print(f"Best cross-validation accuracy (XGBoost): {best_rmse_xgb:.4f}")

xgb_tuned = XGBRegressor(
    max_depth=best_params_xgb['max_depth'],
    learning_rate=best_params_xgb['learning_rate'],
    random_state=42
)
start_time = time.time()
xgb_tuned.fit(X_train_scaled, y_train)
end_time = time.time()
training_time_xgb_tuned = end_time - start_time

xgb_tuned_pred = xgb.predict(X_test_scaled)

rmse_xgb_tuned = np.sqrt(mean_squared_error(y_test, xgb_tuned_pred))
r2_xgb_tuned = r2_score(y_test, xgb_tuned_pred)
mae_xgb_tuned = mean_absolute_error(y_test, xgb_tuned_pred)

print(f"Mean Squared Error (tuned xgb): {rmse_xgb_tuned}")
print(f"MAE (tuned xgb): {mae_xgb_tuned}")
print(f"R-squared (tuned xgb): {r2_xgb_tuned}")

"""## **Gradient Boosting Regressor**"""

gbr = GradientBoostingRegressor(
    n_estimators=300,
    learning_rate=0.05,
    max_depth=3,
    random_state=42
)
start_time = time.time()
gbr.fit(X_train_scaled, y_train)
end_time = time.time()
training_time_gbr = end_time - start_time

gbr_pred = gbr.predict(X_test_scaled)

rmse_gbr = np.sqrt(mean_squared_error(y_test, gbr_pred))
r2_gbr = r2_score(y_test, gbr_pred)
mae_gbr = mean_absolute_error(y_test, gbr_pred)

print(f"Mean Squared Error (gbr): {rmse_gbr}")
print(f"MAE (gbr): {mae_gbr}")
print(f"R-squared (gbr): {r2_gbr}")

param_grid_gbr_reg = {
    'criterion': ['friedman_mse', 'squared_error', 'absolute_error'],
    'n_estimators': [100, 200],
    'learning_rate': [0.01, 0.1],
    'max_depth': [3, 5]
}

grid_search = GridSearchCV(estimator=gbr,
                              param_grid=param_grid_gbr_reg,
                              cv=5,  # Number of cross-validation folds
                              scoring='neg_mean_squared_error',  # Evaluation metric
                              verbose=1)

grid_search.fit(X_train_scaled, y_train)

# Get the best parameters and best score
best_params_gbr = grid_search.best_params_
best_score_gbr = grid_search.best_score_
best_rmse_gbr = np.sqrt(-best_score_gbr)
best_gbr_model=grid_search.best_estimator_
print(f"Best parameters for Decision Tree: {best_params_gbr}")
print(f"Best cross-validation accuracy (Decision Tree): {best_rmse_gbr:.4f}")

gbr_tuned = GradientBoostingRegressor(
    n_estimators=best_params_gbr['n_estimators'],
    learning_rate=best_params_gbr['learning_rate'],
    max_depth=best_params_gbr['max_depth'],
    random_state=42
)
start_time = time.time()
gbr_tuned.fit(X_train_scaled, y_train)
end_time = time.time()
training_time_gbr_tuned= end_time - start_time

gbr_tuned_pred = gbr_tuned.predict(X_test_scaled)

rmse_gbr_tuned = np.sqrt(mean_squared_error(y_test, gbr_tuned_pred))
r2_gbr_tuned = r2_score(y_test, gbr_tuned_pred)
mae_gbr_tuned = mean_absolute_error(y_test, gbr_tuned_pred)

print(f"Mean Squared Error (gbr): {rmse_gbr}")
print(f"MAE (gbr): {mae_gbr}")
print(f"R-squared (gbr): {r2_gbr}")

"""## **Support Vector Regression (SVR)**"""

svr = make_pipeline(
    StandardScaler(),
    SVR(kernel='rbf', C=100, epsilon=0.1)
)
start_time = time.time()
svr.fit(X_train_scaled, y_train)
end_time = time.time()
training_time_svr = end_time - start_time

svr_pred = svr.predict(X_test_scaled)

rmse_svr = np.sqrt(mean_squared_error(y_test, svr_pred))
r2_svr = r2_score(y_test, svr_pred)
mae_svr = mean_absolute_error(y_test, svr_pred)

print(f"RMSE (SVR): {rmse_svr}")
print(f"MAE (SVR): {mae_svr}")
print(f"R-squared (SVR): {r2_svr}")

"""## **Comparison of Results**"""

results_df = pd.DataFrame({
    'Model': [
        'XGBoost',
        'SVR',
        'Random Forest',
        'Gradient Boosting',
        'Decision Tree',
        'Decision Tree (Tuned)',
        'XGBoost (Tuned)',
        'Random Forest (Tuned)',
        'Gradient Boosting Regressor (Tuned)'
    ],
    'RMSE': [rmse_xgb, rmse_svr, rmse_rf, rmse_gbr, rmse_dt, rmse_dt_tuned, rmse_xgb_tuned, rmse_rf_tuned, rmse_gbr_tuned],
    'MAE': [mae_xgb, mae_svr, mae_rf, mae_gbr, mae_dt, mae_dt_tuned, mae_xgb_tuned, mae_rf_tuned, mae_gbr_tuned],
    'R-squared': [r2_xgb, r2_svr, r2_rf, r2_gbr, r2_dt, r2_dt_tuned, r2_xgb_tuned, r2_rf_tuned, r2_gbr_tuned],
    'Training Time (s)':[
        training_time_xgb,
        training_time_svr,
        training_time_dt,
        training_time_gbr,
        training_time_dt,
        training_time_dt_tuned,
        training_time_xgb_tuned,
        training_time_rf_tuned,
        training_time_gbr_tuned
    ]
})

results_df = results_df.sort_values(by='R-squared', ascending=False).reset_index(drop=True)
display(results_df)

import matplotlib.pyplot as plt
import numpy as np

# Assuming y_test and all prediction variables (dt_pred, rf_pred, etc.) are available from previous cells

plt.figure(figsize=(15, 8))

# Plot actual values
plt.plot(np.arange(len(y_test)), y_test.values, label='Actual Prices', color='black', linewidth=2)

# Plot predictions from each model
plt.plot(np.arange(len(y_test)), dt_pred, label='Decision Tree Predictions (Untuned)', linestyle='--')
plt.plot(np.arange(len(y_test)), dt_tuned_pred, label='Decision Tree Predictions (Tuned)', linestyle='-.')
plt.plot(np.arange(len(y_test)), rf_pred, label='Random Forest Predictions (Untuned)', linestyle=':')
plt.plot(np.arange(len(y_test)), rf_tuned_pred, label='Random Forest Predictions (Tuned)', linestyle='--')
plt.plot(np.arange(len(y_test)), xgb_pred, label='XGBoost Predictions (Untuned)', linestyle='-')
plt.plot(np.arange(len(y_test)), xgb_tuned_pred, label='XGBoost Predictions (Tuned)', linestyle='-.')
plt.plot(np.arange(len(y_test)), gbr_pred, label='Gradient Boosting Predictions (Untuned)', linestyle=':')
plt.plot(np.arange(len(y_test)), gbr_tuned_pred, label='Gradient Boosting Predictions (Tuned)', linestyle='--')
plt.plot(np.arange(len(y_test)), svr_pred, label='SVR Predictions', linestyle='-')

plt.title('Comparison of Actual vs. Predicted Prices by Model')
plt.xlabel('Sample Index')
plt.ylabel('Price')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Helper function to plot actual vs. predicted for a single model
def plot_model_predictions(actual_prices, untuned_preds, tuned_preds=None, model_name='Model'):
    plt.figure(figsize=(12, 6))
    plt.plot(np.arange(len(actual_prices)), actual_prices.values, label='Actual Prices', color='black', linewidth=2)
    plt.plot(np.arange(len(actual_prices)), untuned_preds, label=f'{model_name} (Untuned) Predictions', linestyle='--')
    if tuned_preds is not None:
        plt.plot(np.arange(len(actual_prices)), tuned_preds, label=f'{model_name} (Tuned) Predictions', linestyle='-.')

    plt.title(f'Actual vs. Predicted Prices: {model_name}')
    plt.xlabel('Sample Index')
    plt.ylabel('Price')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

# Decision Tree Regressor
plot_model_predictions(y_test, dt_pred, dt_tuned_pred, model_name='Decision Tree')

# Random Forest Regressor
plot_model_predictions(y_test, rf_pred, rf_tuned_pred, model_name='Random Forest')

# XGBoost
plot_model_predictions(y_test, xgb_pred, xgb_tuned_pred, model_name='XGBoost')

# Gradient Boosting Regressor
plot_model_predictions(y_test, gbr_pred, gbr_tuned_pred, model_name='Gradient Boosting')

# SVR (only untuned version)
plot_model_predictions(y_test, svr_pred, model_name='SVR')